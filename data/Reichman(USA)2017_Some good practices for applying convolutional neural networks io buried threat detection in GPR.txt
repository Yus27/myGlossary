Reichman
Some Good Practices for Applying Convolutional Neural Networks to Buried Threat Detection in Ground Penetrating Radar
2017
IEEE

Some Good Practices for Applying Convolutional Neural Networks to Buried Threat Detection in Ground Penetrating Radar Daniël Reichman, Leslie M. Collins, Jordan M. Malof Electrical and Computer Engineering Duke University Durham, North Carolina, USA Abstract—Ground Penetrating Radar (GPR) is a remote sensing modality that has been researched extensively for buried threat detection. For this purpose, algorithms can be developed to automatically determine the presence of such threats. To train such algorithms, small 2-dimensional images can be extracted from the larger image, or volume, of GPR data. One thread of research in the buried threat detection literature is to use visual descriptors from the computer vision literature. One recent, very successful approach in that field is the use of deep convolutional neural networks (CNNs). Applying CNNs requires a large number of design choices which complicate their use. In this work, we investigate their application to GPR data and adapt several recent advances from the CNN literature to improve detection performance on GPR data. In particular, we investigate the initialization step of pretraining and propose a dataset augmentation protocol. The efficacy of these approaches are evaluated on several architectures with a relatively similar number of network parameters to learn. The results indicate that both pretraining and dataset augmentation help achieve higher detection performance.  Recently, deep convolutional neural networks (CNNs) have achieved impressive performance for image recognition tasks on natural images [16], [17]. This suggests that CNNs may also yield improvements for threat recognition in GPR data. CNNs, however, exhibit several characteristics that make their application to GPR buried threat detection (BTD) difficult. To achieve good performance with a CNN, an appropriate network architecture has to be specified [18], [19], which requires a large number of design decisions. This problem is exacerbated by the typically large amounts of data necessary to train the network, and the resulting long training times. For GPR in particular, obtaining sufficient amounts of data to train a network could be prohibitive. This was cited as a major difficulty in several previous applications of CNNs to BTD in downward looking GPR data [13], [20]–[22].  Keywords— convolutional neural networks, ground penetrating radar, buried threat detection, pretraining, augmentation  I. INTRODUCTION The ground penetrating radar (GPR) is a remote sensing technology with which state-of-the-art performance has been achieved for detecting buried explosive threats [1]–[7]. The GPR considered here is comprised of an array of antennas. Each antenna emits an electromagnetic pulse into the ground resulting in a time-series, known as an A-scan [8], at every queried location. Consecutive A-scans can be concatenated to create images of the subsurface, called B-scans. Buried threats exhibit characteristic patterns in B-scans, such as hyperbolas, which can be leveraged for detection [8]–[11]. Examples of Bscans collected over threats are shown in Figure 1. A large body of published research has focused on developing algorithms that automatically detect buried threats in GPR data. Many of these algorithms operate on small images extracted from larger GPR volumes, or B-scans. Perhaps as a result of this, many of the GPR detection algorithms leverage image recognition techniques from the computer vision literature [8], [12], [13]. Some examples include Histogram of Oriented gradients (HOG) [8], [9], Edge Histogram Descriptors (EHD) [14], [15], and Scale Invariant Feature Transform [12].  Figure 1: B-scan images of GPR data collected over three buried threats.  In this work we adapt several recent advances in the CNN literature to improve the performance of CNNs on GPR data. We use a dataset of GPR data to measure the performance of the CNN detector after each modification, confirming their respective benefits. In particular, we investigate the following modifications: • We use grayscale imagery from the Cifar10 [23] dataset to pre-train our GPR CNN. • We use a data augmentation procedure to increase the amount of useful GPR training data. Also, motivated by recent CNN research, we universally employ small 3 × 3 pixel convolutional filters [19]. If (and only if) we make all modifications, we can effectively train deeper CNNs than have previously been presented in the GPR literature. Several of our architectures outperform a state-ofthe-art GPR BTD algorithm, consisting of a Random Forest classifier and Histogram of Oriented Gradient (HOG) features.  This paper is organized as follows: in Section II, the dataset for these experiments is described. In Section III, background information about CNNs is provided. In Section IV, proposed training improvements for GPR are described. In Section V, the results of this investigation are presented. In Section VI, conclusions about this work are drawn. II. GPR DATASET DESCRIPTION The data used in this work was measured using a vehicle mounted array of downward looking GPRs, very similar to the detection system considered in many previous studies [1], [2], [24], [25]. The data is collected at regular intervals on the path of the vehicle, resulting in a 3D volume. The data was collected at a western U.S. test site over a total area of 167,166.1 m2. 73 runs were made over 8 distinct lanes for a total of 1,932 threat encounters. To obtain training data for threats and non-threats from the 3D volume, locations of interest, also referred to as keypoints [26], are identified. First, spatial keypoints are identified with a computationally inexpensive anomaly detector known as a prescreener. A prescreener outputs a confidence of threat presence at every spatial location in the queried area. A sensitivity threshold is set and all locations with a confidence above that threshold are declared alarms, i.e., locations to inspect further. In this work, a variant of the energy-based, F1V4 prescreener [27] is used to identify spatial locations of interest, with a threshold set such that 6558 total alarms are declared. This yielded a detection rate of 96.8%, resulting in 1,871 true threat detection and 4687 false alarms.  Figure 2: Examples of × pixel patches extracted at temporal locations of maximum energy at prescreener alarm locations for buried threats. A dataset of such patches is used for training and testing the CNN.  As can be seen in Figure 1, the GPR data relevant for training have a limited temporal extent. For this reason, a temporal estimation method, first introduced in [10], is used to identify good temporal locations to extract training patches. At each spatial alarm location, 18 × 18 pixel patches of data are extracted at temporal locations of local maxima of the signal energy. Before extracting patches, the data is down sampled by a factor of 2 and background normalized. Background normalization is conducted by estimating a mean and standard deviation of the background at each time index using data surrounding the central A-scan in the B-scan. Examples of the resulting patches are shown in Figure 2. In this work, the training and testing is performed according to a protocol specifically designed for supervised algorithm  development for BTD in GPR data [26]. The training dataset consists of patches extracted at the top 4 (temporal) energy locations over threats; and at small, regular temporal intervals, for non-threats. At test time, the classifier tests at small regular temporal intervals on the central A-scan of the prescreener alarm locations. The final confidence at a prescreener alarm is the sum of the top 12 decision statistics (i.e., classifier outputs). III. CONVOLUTIONAL NEURAL NETWORK BACKGROUND A CNN is comprised of a set of learned feature extractors connected to a neural network classifier. This is unlike traditional object detection pipelines which typically separate the extraction of a static set of features (e.g., HOG [28] or SIFT [29]) and the classification of the data. The CNN is comprised of layers that are connected in a hierarchical fashion. The first layers typically consist of a combination of convolutional filters, spatial pooling layers, and normalizations. The final few layers form an artificial neural network classifier which ultimately returns a probability for each possible class of the input (e.g., threat or non-threat). A. Parameter optimization The parameters for all layers of a CNN are learned jointly using stochastic gradient descent (SGD) [16]. In training, the dataset is provided to the network in mini-batches (i.e., small, randomly selected subsets) and the whole dataset is traversed times (epochs). To update the weights, the network is evaluated on a mini-batch of data, the prediction is compared to the training labels, and the error is back-propagated through the network. At the end of an epoch, the network is typically evaluated on a validation set to measure whether the network is learning general patterns or overfitting to the training data [19]. The final network chosen for testing is the one with highest validation accuracy. B. Xavier-plus parameter initialization The speed of convergence of the network also depends on the initialization of the network weights [19]. This includes the weights in the convolutional filters as well as those in the fully connected layers. A simple solution to this is a random initialization of the weights, drawn from a Gaussian [19]. Recently, a method has been suggested to compute the variance of the Gaussian from which to draw the random weights and is known as the Xavier initialization method [30]. This method was further developed in [31] to apply when linear rectified units (ReLu) are employed as the non-linearity at the output of each layer. This method is referred to here as Xavier-plus and chooses an appropriate scale for the initial weights based on the specific network’s topology. C. General regularization approaches To reduce the possible overfitting of the large number of parameters in the network to the training data, many methods have been developed [32]. In this work, we employ two of the most common methods for this purpose. The first is to penalty on the weights [33]. The second is introduce an called dropout [32], in which, in each epoch, a certain proportion of the nodes in the fully connected layers are  randomly selected not to be updated. Thus, each node, in effect, sees a different subset of the data during training, but all the nodes are used together during testing, resulting in a modelaveraging effect. IV. SOME GOOD PRACTICES FOR APPLYING CONVOLUTIONAL NEURAL NETWORKS TO GPR DATA In this section we describe our experiments adapting some more recent techniques from the CNN literature to GPR data. With the exception of the approach in IV.A, we perform controlled experiments showing that each of these approaches is beneficial for our problem. A. Smaller convolutional filters To reduce the number of parameters to learn, the Visual Geometry Group at Oxford University suggest splitting larger filters into several smaller filters in series [19]. For example, instead of learning a 5 × 5 pixel filter, they suggest learning two 3 × 3 pixel filters. They also show that to improve results, a non-linear unit such as the ReLu should be added between the two 3 × 3 filters. B. Network pretraining A recent finding in the CNN literature is that the filters learned at the early stages of the network are often very general (e.g., edges and blobs) [17]. In [17], [19], the filters of one network were used to initialize a network on a different task, leading to improved performance. Using a pre-trained network in this way is also possible in GPR and would increase the “effective” number of data points the network has seen. In this work, we first learn a network on the popular Cifar10 dataset [23], which, in order to match our single channel patches, is converted to grayscale. Cifar10 was chosen because, due to its relatively small size, it is faster and easier to train a custom grayscale CNN compared to other popular datasets like ImageNet [16]. The topology of the network being trained on the Cifar10 dataset has the same number of filters in the first few layers as the network that will be used on the GPR data. Pretraining here was done by transferring at most 4 convolutional layers from the network trained on the Cifar10 dataset to the GPR network. This was done following the guidelines in [17] which suggest that the parameters in deeper layers become very specific to the dataset on which that network was trained. This may be detrimental here due to the dissimilarity between natural images and GPR data. C. Training data augmentation Another option to increase the dataset size is to augment the dataset with transformations of the data. For example, in natural images, objects can appear rotated or at different scales of illumination and therefore, adding rotated and scaled patches to the dataset has been shown to improve classification performance [16], [34]. In the context of GPR data, those transformations are not appropriate, however, there may exist other augmentation strategies. As can be seen in the B-scans in Figure 1, the threat signature can extend many time samples and extracting a bounding box of fixed size for the entire dataset can be  challenging. For this reason, we suggest extracting patches at several offsets from the maximum signal energy location. The resulting training dataset thus consists of patches extracted down the depth for non-threats and then extracting patches at the maximum energy locations and their surrounding within a range for threats. Note that augmenting vertically only affects threats (because for non-threats, vertical patches are taken down the depth). Horizontal augmentation affects both threats and non-threats. At test time, patches from all columns within the horizontal radius of the augmentation procedure are considered. In this work, the data augmentation parameters are the following. In the vertical dimension, patches at every other pixel above and below the maximum keypoint location for threats are chosen, up to 4 pixels away. In the horizontal dimension, one patch is chosen 2 pixels away, in both directions from the maximum energy keypoint for threats. For nonthreats, 33% of the data is randomly sampled from 3 A-scans: the central A-scan, one A-scan 2 pixels to the right and to the left of the central A-scan. Augmenting the dataset in the proposed way also affects the testing procedure outlined in [26], i.e., testing down the depth of an alarm’s B-scan and summing the top 12 confidences. Using cross-validation on several networks not shown in this work due to space limitations, the number of confidences over which to sum was found to be 49 confidences. V. RESULTS In this section we begin by describing specifics of the patch dataset used, followed by the experimental design and a presentation of the results. A. Data preprocessing For the experiments in this work, several preprocessing steps are applied to the dataset of patches. Unlike natural images, GPR data is susceptible to large variations in signal amplitude, depending on specific soil conditions and depth propagation effects. For this reason, the background mean and standard deviation at each time index is estimated and removed. In addition, the patch-mean of the 18 × 18 pixel patches is removed from each patch. Note that the patch-mean is computed on the training dataset only and that same patch-mean is removed from the testing dataset. B. Experimental design To validate the efficacy of network pretraining and dataset augmentation in this context, several design decisions have to be made. These regard the networks used, the cross-validation procedure used, and how they are trained. The CNN architectures we consider in our experiments are presented in Table 1. The architectures were designed to keep the number of parameters in each network relatively similar and then convolutional layers are traded for fully connected layers. The specific number of parameters for each architecture can be found in Table 2. Cross-validation is performed to ensure that the performance reported is valid. Here we split the 8 lanes into 4 groups and train on 6 lanes and test on the 2 lanes being held out. In addition, the validation set, which is typically chosen at  random, has to be designed carefully. The validation set is constructed by holding out 1 out of the 6 lanes in training. Table 1: Description of the 3 networks employed in this work. “conv3-16” refers to a convolutional layer with 16 filters of ! × ! pixels. Maxpool(2x2, 2) refers to a pooling layer where the maximal filter response at each 2nd pixel is chosen within a window of 2× " pixels. Finally, the fully connected layers (FC-32) represent the number of neurons (e.g., 32) in that classification layer. Note that a ReLu unit is placed after each convolutional layer (not shown here).  CNN configurations Input (18 × 18 image) (b) Conv3-16 Conv3-16 Maxpool(2 × 2, 2) Conv3-32 Conv3-32 Conv3-32 Conv3-32 Maxpool(2 × 2, 2) (a) Conv3-16 Conv3-16  (c) Conv3-24 Conv3-24  Conv3-64 Conv3-64 Maxpool(2 × 2, 2) FC-32 FC-32  FC-64 FC-16 Soft-max  To train the network, the data is processed in mini-batches of 32	samples for a total of 16 epochs. To update the weights, we employ SGD with momentum with an	 regularization factor of 10 and a momentum factor of	0.9. A dropout rate of 50% is defined for each fully connected layer in the network. The learning rate is diminished every 4 epochs, the specific values being	[10 , 7 × 10 , 3 × 10 , 10 ]. After each epoch, the network is tested on a validation set, to measure the performance of the network trained up to that point. Table 2: Number of parameters for each network architecture (in thousands)  Network Number of parameters  (a)  (b)  (c)  88  91  83  Choosing the network to use during testing, e.g., one that did not overfit to the training data and will generalize to the testing data, is a challenging problem [18], [32], [35], [36]. In our experiments, we have found that the performance measured after each epoch on the validation set is not representative of the performance on the test data. This could be, in part, due to the differences in soil composition and threat distributions between the lanes in our dataset. Addressing this problem is beyond the scope of this paper and is left as future work. In an attempt to minimize a positive bias on the results, we report here the average AUC obtained by testing the data with the resulting networks in epochs	5 − 10 inclusive.  C. Results of comparison The results from this comparison are shown in Table 3 and are reported as the area under the ROC curve (AUC) up to a false alarm rate (FAR) of	0.025	 / . A baseline for performance on this data is the previously published state-ofthe-art algorithm using the HOG feature with the random forest classifier [8] which is shown in the final column of the table. Note that no pre-training is possible with the random forest, but the AUC when classifying the augmented dataset is shown in the final column in the table. The results from this comparison suggest that the training procedures outlined here improve performance as they do in natural images. While initializing the network with Xavier-plus initialization achieves high detection performance, it is further benefited both by initializing the convolutional layers with pretrained layers and additionally by the dataset augmentation. Note that the HOG+RF performance did not improve with the augmented dataset. Table 3: Classification performance in terms of the average AUC is shown when testing with the trained networks from epochs − for the CNN architectures defined in Table 1 along with a baseline algorithm. These algorithms are compared using the training procedures outlined in Section IV. Training type  (a)  (b)  (c)  HOG + RF  Xavier-plus  0.9374  0.9383  0.9360  0.9254  Pre-trained Pre-trained + Augmentation  0.9398  0.9422  0.9401  --  0.9442  0.9471  0.9451  0.9226  Finally, we remark on the choice of architectures in this work. Architecture (b), the best performing network in each row, consists of 2 sets of double-convolutional layers and 2 fully connected layers. This results leaves it unclear whether performance is improved because of a powerful classifier or more expressive features. However, all three architectures have a similar number of parameters and the performance differences are not very large. This suggests that high detection performance is possible with a deep CNN when used with pretraining and augmentation. VI. CONCLUSIONS In this work, we investigate the application of convolutional neural networks to the problem of BTD in GPR data. A central challenge presented by CNNs on GPR is limited training data, which can result in poor detection performance. Many solutions have been developed in the broader computer vision community for this problem, and here we adapted two of these solutions to buried threat detection in GPR data: network pre-training and data augmentation. Both of these strategies yielded improvements in the detection performance of the CNNs considered here. Another challenge of CNNs is identifying effective architectures for the problem under consideration. Here we investigated three architectures, all of which yielded improvements in detection performance over a state-of-the-art detector involving a Random Forest classifier and Histogram of Oriented Gradient (HOG) feature.  A. Limitations and future work Although the presented results are promising, the results here were obtained on a single GPR dataset, with its associated characteristics, sampling rates, and the particular conditions in the soil at the time of collection. They are also limited by specific experimental design choices such as the size of GPR image patches (18 × 18 pixels), and the particular crossvalidation procedure. Further experiments on independently collected GPR data (perhaps from other sensors or synthetically generated), with alternative experimental designs, would be beneficial in demonstrating the generality of these findings.  ACKNOWLEDGEMENTS This work was supported by the U.S. Army RDECOM CERDEC Night Vision and Electronic Sensors Directorate, via a Grant Administered by the Army Research Office under Grant W911NF-06-1-0357 and Grant W911NF-13-1-0065. Additionally, we gratefully acknowledge NVIDIA for the donation of a GPU toward this research.  REFERENCES [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10]  [11]  [12]  [13]  J. N. Wilson, P. D. Gader, W. Lee, H. Frigui, and K. C. Ho, “A LargeScale Systematic Evaluation of Algorithms Using Ground-Penetrating Radar for Landmine Detection and Discrimination,” IEEE Trans. Geosci. Remote Sens., vol. 45, no. 8, pp. 2560–2572, 2007. P. D. Gader, M. Mystkowski, and Y. Zhao, “Landmine Detection with Ground Penetrating Radar Using Hidden Markov Models,” IEEE Trans. Geosci. Remote Sens., vol. 39, no. 6, pp. 1231–1244, 2001. R. J. Stanley, P. D. Gader, and K. C. Ho, “Feature and decision level sensor fusion of electromagnetic induction and ground penetrating radar sensors for landmine detection with hand-held units,” Inf. Fusion, vol. 3, no. 3, pp. 215–233, 2002. Q. Zhu and L. M. Collins, “Application of feature extraction methods for landmine detection using the Wichmann/Niitek ground-penetrating radar,” IEEE Trans. Geosci. Remote Sens., vol. 43, no. 1, pp. 81–85, 2005. V. Kovalenko, A. G. Yarovoy, and L. P. Ligthart, “Alternating-Sign Windowed Energy Projection of SAR Focused GPR Data,” Eur. Radar Conf., vol. 3, pp. 415–418, 2005. V. Kovalenko, A. G. Yarovoy, and L. P. Ligthart, “A novel clutter suppression algorithm for landmine detection with GPR,” IEEE Trans. Geosci. Remote Sens., vol. 45, no. 11, pp. 3740–3750, 2007. E. Pasolli, F. Melgani, and M. Donelli, “Automatic analysis of GPR images: A pattern-recognition approach,” IEEE Trans. Geosci. Remote Sens., vol. 47, no. 7, pp. 2206–2217, 2009. P. A. Torrione, K. D. Morton, R. T. Sakaguchi, and L. M. Collins, “Histograms of Oriented Gradients for Landmine Detection in GroundPenetrating Radar Data,” IEEE Trans. Geosci. Remote Sens., vol. 52, no. 3, pp. 1539–1550, Mar. 2014. K. L. Lee and M. M. Mokji, “Automatic Target Detection in GPR Images Using Histogram of Oriented Gradients ( HOG ),” in Electronic Design (ICED), 2014 2nd International Conference on. IEEE, 2014, pp. 181– 186. P. A. Torrione, L. M. Collins, and S. Member, “Texture Features for Antitank Landmine Detection Using Ground Penetrating Radar,” IEEE Trans. Geosci. Remote Sens., vol. 45, no. 7, pp. 2374–2382, 2007. R. N. Nagashree, N. Aswini, A. Dyana, and C. H. Srinivas Rao, “Detection and Classification of Ground Penetrating Radar image using textrual features,” in International Conference on Advances in Electronics, Computers and Communications, 2014. R. T. Sakaguchi, J. Morton Kenneth D., L. M. Collins, and P. A. Torrione, “Keypoint-based image processing for landmine detection in GPR data,” Proc. SPIE 8357, Detect. Sens. Mines, Explos. Objects, Obs. Targets XVII, vol. 8357, p. 83571Z–83571Z–11, 2012. R. T. Sakaguchi, K. D. Morton, L. M. Collins, and P. A. Torrione,  [14]  [15]  [16]  [17]  [18]  [19] [20]  [21] [22]  [23] [24]  [25]  [26]  [27]  [28]  [29] [30]  [31]  [32]  [33] [34]  [35]  [36]  Recognizing subsurface target responses in ground penetrating radar data using convolutional neural networks,” in SPIE Defense+ Security, 2015, vol. 9454, p. 94541A. H. Frigui, O. Missaoui, and P. D. Gader, “Landmine Detection using Discrete Hidden Markov Models with Gabor Features,” in SPIE Defense+ Security, 2007, vol. 6553, p. 65532A–65532A–10. O. Missaoui, H. Frigui, and P. D. Gader, “Model Level Fusion of Edge Histogram Descriptors and Gabor Wavelets,” in Geoscience and Remote Sensing Symposium (IGARSS), 2010 IEEE International. IEEE, 2010, pp. 3378–3381. A. Krizhevsky, Ii. Sulskever, and G. E. Hinton, “ImageNet Classification with Deep Convolutional Neural Networks,” Adv. Neural Inf. Process. Syst., pp. 1–9, 2012. M. Oquab, M. Oquab, I. Laptev, J. S. Learning, T. Mid-level, M. Oquab, and L. Bottou, “Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks,” in CVPR, 2014. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabinovich, C. Hill, and A. Arbor, “Going Deeper with Convolutions,” 2014. K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” Iclr, pp. 1–14, 2015. L. E. Besaw and P. J. Stimac, “Deep convolutional neural networks for classifying GPR B-scans,” Proc. SPIE - Int. Soc. Opt. Eng., vol. 9454, p. 945413, 2015. L. E. Besaw, “Detecting buried explosive hazards with handheld GPR and deep learning,” vol. 9823, p. 98230N, 2016. J. Bralich, D. Reichman, L. M. Collins, and J. M. Malof, “Improving Convolutional Neural Networks for Buried Target Detection in Ground Penetrating Radar Using Transfer Learning Via Pre-training,” in SPIE Defense, Security, and Sensing, 2017. A. Krizhevsky, “Learning Multiple Layers of Features from Tiny Images,” … Sci. Dep. Univ. Toronto, Tech. …, pp. 1–60, 2009. H. Frigui and P. Gader, “Detection and discrimination of land mines based on edge histogram descriptors and fuzzy K-nearest neighbors,” IEEE Int. Conf. Fuzzy Syst., pp. 1494–1499, 2006. C. R. Ratto, K. D. M. Jr, L. M. Collins, S. Member, and P. A. Torrione, “Bayesian Context-Dependent Learning for Anomaly Classification in Hyperspectral Imagery,” IEEE Trans. Geosci. Remote Sens., vol. 52, no. 4, pp. 1969–1981, 2014. D. Reichman, L. M. Collins, and J. M. Malof, “On Choosing Training and Testing Data for Supervised Algorithms in Ground Penetrating Radar Data for Buried Threat Detection,” arXiv: 1612.03477, 2016. P. A. Torrione, C. S. Throckmorton, and L. M. Collins, “Performance of an Adaptive Feature-Based Processor for a Wideband Ground Penetrating Radar System,” Trans. Aerosp. Electron. Syst., vol. 42, no. 2, 2006. N. Dalal and B. Triggs, “Histograms of Oriented Gradients for Human Detection,” CVPR ’05 Proc. 2005 IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. - Vol. 1, pp. 886–893, 2005. D. G. Lowe, “Distinctive image features from scale invariant keypoints,” Int’l J. Comput. Vis., vol. 60, pp. 91–11020042, 2004. X. Glorot and Y. Bengio, “Understanding the difficulty of training deep feedforward neural networks,” Proc. 13th Int. Conf. Artif. Intell. Stat., vol. 9, pp. 249–256, 2010. K. He, X. Zhang, S. Ren, and J. Sun, “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,” CoRR, vol. abs/1502.0, 2015. N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, “Dropout: prevent NN from overfitting,” J. Mach. Learn. Res., vol. 15, pp. 1929–1958, 2014. S. J. Nowlan and G. E. Hinton, “Simplifying Neural Networks by Soft Weight-Sharing,” Neural Comput., vol. 4, no. 4, pp. 473–493, 1992. P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun, “OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks,” arXiv Prepr. arXiv, p. 1312.6229, 2013. S. Ioffe and C. Szegedy, “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,” arXiv:1502.03167, pp. 1–11, 2015. M. D. Zeiler and R. Fergus, “Stochastic Pooling for Regularization of Deep Convolutional Neural Networks,” Iclr, pp. 1–9, 2013.  