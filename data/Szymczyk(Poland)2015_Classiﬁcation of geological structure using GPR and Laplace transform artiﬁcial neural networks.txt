Szymczyk
Classiﬁcation of geological structure using ground penetrating radar and Laplace transform artiﬁcial neural networks
2015
Neurocomputing 148 (2015) 354–362  

Contents lists available at ScienceDirect  Neurocomputing journal homepage: www.elsevier.com/locate/neucom  Classiﬁcation of geological structure using ground penetrating radar and Laplace transform artiﬁcial neural networks P. Szymczyk n, M. Szymczyk AGH University of Science and Technology, The Faculty of Electrical Engineering, Automatics, Computer Science and Biomedical Engineering, Department of Automatics and Biomedical Engineering, al. A. Mickiewicza 30, 30-059 Krakow, Poland  art ic l e i nf o  a b s t r a c t  Article history: Received 6 April 2014 Received in revised form 11 May 2014 Accepted 8 June 2014 Communicated by R. Tadeusiewicz Available online 19 June 2014  This paper focuses on a new kind of artiﬁcial neural networks – the Laplace transform artiﬁcial neural networks (LTANN). It is proposed to use the Laplace transform instead of ordinary weights and a linear activation function of an artiﬁcial neuron. This extension allows to use artiﬁcial neural networks in new areas. The ordinary description of artiﬁcial neural networks is a special case of the description proposed in this paper. Three models of different geological structures based on the LTANN are discussed in this paper. Using these models, it is possible to classify an unknown geological structure as the structure without anomaly, the structure with a sinkhole or the structure with a loose zone. & 2014 Elsevier B.V. All rights reserved.  Keywords: Neural networks Laplace transform Laplace transform artiﬁcial neural network LTANN Ground penetrating radar Classiﬁcation of a geological structure  1. Introduction 1.1. Problem with interpretation and recognizing of ground penetrating radar signal GPR (Ground Penetrating Radar) has been widely used for almost twenty years in archeology, geology, and civil engineering [1,2]. GPR sends EM waves in the ground and then collects backscattered echoes (Fig. 1). GPR acquires signal traces moving along survey line. These raw data are then collected to form 2D GPR proﬁle (radargram) [3]. The example of radargram is shown in Fig. 2, the example of transmitted signal is shown in Fig. 3 and the example of one trace is shown in Fig. 4. GPR signal is difﬁcult to interpret because it is disturbed and distorted. Attenuation of electromagnetic waves in a real geological medium is often large, and the power of GPR signal must be small for safety reasons. GPR method is sensitive to noises from other devices, such as mobile phones, GSM, WiFi, Bluetooth, all kinds of cables and wires for power and data transmission. GPR is also susceptible to numerous disturbances caused, for example by the presence of nearby buildings, rails, pipes, various metal objects, strong electromagnetic  calls, etc. The terrain (uneven ground, troughs, depressions, vegetation, trees, landscaping elements) impede and distort the measurement results. They are very much dependent on the degree of saturation of the soil with water so they depend on the amount of rainfall in the period immediately preceding the measurements. Geological structure is practically always composed of different materials, the interporous spaces (if any) are ﬁlled with water (with different mineral composition) or gas (usually air). The quality of recorded results is dependent on the choice of measurement parameters of GPR, which are often difﬁcult to be optimally deﬁned. Interpretation of radargrams is performed by a specialist, based on their experience and intuition. It is often affected by error. There is no unambiguous automatic recognition method of GPR records. The use of neural networks to solve the described problem appears promising. Typical neural networks to solve this task are not very suitable because of the difﬁculty of taking into account the dynamics of the problem. Therefore, the paper attempts to apply a new type of neural network taking into consideration the dynamics of the waveforms to solve a given problem – LTANN network.  1.2. Classiﬁcation problem n  Corresponding author. E-mail addresses: piotr.szymczyk@agh.edu.pl (P. Szymczyk), magdalena.szymczyk@agh.edu.pl (M. Szymczyk). http://dx.doi.org/10.1016/j.neucom.2014.06.025 0925-2312/& 2014 Elsevier B.V. All rights reserved.  Classiﬁcation is a problem of identifying to which set of categories a new observation belongs, on the basis of a training set of data  P. Szymczyk, M. Szymczyk / Neurocomputing 148 (2015) 354–362  355   Linear classiﬁers  Fig. 1. GPR system.  Fig. 2. The radargram.             ○ Fisher's linear discriminant ○ Logistic regression ○ Naive Bayes classiﬁer ○ Perceptron Support vector machines Quadratic classiﬁers Kernel estimation (k-nearest neighbor) Boosting (meta-algorithm) Decision trees (Random forests) Neural networks Gene Expression Programming Bayesian networks Hidden Markov models Learning vector quantization  Each classiﬁer has its own strengths and weaknesses. The most widely used classiﬁers are the neural networks. Neural networks have been selected for further consideration because they are simple, easy to build, efﬁcient and reliable classiﬁer. They are also give good results. There are many tools for building, using and analyzing neural networks, for example Matlab, Statistica, etc. 1.3. The simple artiﬁcial neuron In Fig. 5 the simple artiﬁcial neuron with n þ 1 inputs x0 ; x1 ; …; xn is shown. Each input has the corresponding weight w0 ; w1 ; …; wn . The neuron has a bias b ¼ w0 x0 . It can be written as the following equation: n  ð1Þ  u ¼ ∑ w i xi i¼0  Fig. 3. The transmitted signal.  and the formula for the neuron output: y ¼ f ðuÞ  ð2Þ  where: ithe number of given input, nthe number of inputs. If we put Eqs. (1) into (2), we obtain: ! n  y¼f  ∑ w i xi  ð3Þ  i¼0  Fig. 4. The example trace from radargram.  The transfer function in Fig. 5 and in Eqs. (2) and (3) may be a linear or a nonlinear function of variable u. A particular transfer function is chosen to satisfy some speciﬁcation of the problem that the neuron attempts to solve [6,7]. There are a lot of different transfer functions, but three of the most commonly used functions are shown below:   Hard limit transfer function: (  y¼  0  for u o0  1  for u Z0  ð4Þ   Linear transfer function: y¼u  ð5Þ  Fig. 5. The simple artiﬁcial neuron.   Log-Sigmoid transfer function: containing observations whose category membership is known. An algorithm that implements classiﬁcation is known as a classiﬁer. The examples of classiﬁcation algorithms are listed below [4,5]:  y¼  1 1þeu  ð6Þ  356  P. Szymczyk, M. Szymczyk / Neurocomputing 148 (2015) 354–362  1.4. The artiﬁcial neural network It is common knowledge that the artiﬁcial neural network is a set of artiﬁcial neurons connected together to form a network which mimics a biological neural network [6–9]. There are a lot of types of artiﬁcial neural networks (ANN). The most popular and the best-known ANN are listed below:            Feedforward neural network [10] Radial basis function (RBF) network [11] Kohonen self-organizing network [12,13] Learning Vector Quantization [14] Recurrent neural network ○ Fully recurrent network [15–20] ○ Hopﬁeld network [13] ○ Boltzmann machine ○ Simple recurrent networks [21] ○ Echo state network [22,23] ○ Long short term memory network [24–26] ○ Bi-directional RNN [27,28] ○ Hierarchical RNN [29] ○ Stochastic neural networks [13] Modular neural networks [13] ○ Committee of machines ○ Associative neural network (ASNN) Physical neural network [30] Other types of networks ○ Holographic associative memory ○ Instantaneously trained networks ○ Spiking neural networks [31] ○ Dynamic neural networks [32,33] ○ Cascading neural networks ○ Neuro-fuzzy networks [13] ○ Compositional pattern-producing networks ○ One-shot associative memory [34]  Therefore, three elements are particularly important in every model of artiﬁcial neural networks [13]:   The structure of the nodes  The topology of the network  The learning algorithm used to ﬁnd the weights of the net work  The learning algorithm is an adaptive method by which a network of computing units organizes itself to implement the desired behavior. This is performed in some learning algorithms by presenting some examples of the desired input–output mapping of the network. A correction step is executed iteratively until the network learns to produce the desired response. The learning algorithm is a closed loop of presentation of examples and corrections of the network parameters [13]. Learning algorithms can be divided into:   The supervised learning   ○ The corrective learning ○ The reinforcement learning The unsupervised learning  Fig. 6. The time-delay neural network for one-dimensional input/output signals.  Fig. 7. The structure of the classiﬁer - S-Transform and artiﬁcial neural networks.  The backpropagation learning algorithm [13] is very important in multilayer networks. Artiﬁcial neural networks are widely spread and used in everyday services, products and applications [35]. In the research [32] a methodology to model the nonlinear deterministic dynamics from a signal is established. This approach is called dynamic modeling. Due to their plasticity and powerful function approximation capability, artiﬁcial neural networks (ANNs) are chosen as the nonlinear modeling tool in that work (Fig. 6). The paper [36] is focused on artiﬁcial neural networks in the realm of modeling, identiﬁcation and control of nonlinear systems. The basic ideas and techniques of artiﬁcial neural networks are presented in the language and notation familiar to control engineers. Different applications of neural networks architectures in control have been surveyed. Applications of artiﬁcial neural network can be found [37] as classiﬁers. This network is thought to recognize features obtained from S-Transform. This case is shown in Fig. 7. It can be noted that artiﬁcial neural networks are used with different kind of transform, but not exactly as a description of an artiﬁcial neuron as it is proposed in this paper.  The artiﬁcial neural network can consist of many neuron layers:   The input layer  The hidden layers  The output layer  2. The Laplace transform artiﬁcial neuron The Laplace transform artiﬁcial neuron presented in Fig. 8 can be seen in its mathematical description below. If we assume that:  P. Szymczyk, M. Szymczyk / Neurocomputing 148 (2015) 354–362  357  Fig. 8. The Laplace transform artiﬁcial neuron.  Fig. 10. The supervised learning LTAN.  Fig. 9. The Laplace transform artiﬁcial neuron after simpliﬁcation.  X i ðsÞ ¼ Lfxi ðtÞg, UðsÞ ¼ LfuðtÞg and YðsÞ ¼ LfyðtÞg, we can write it as: n  UðsÞ ¼ ∑ GXUi ðsÞX i ðsÞ  ð7Þ  i¼0  and YðsÞ ¼ GUY ðsÞUðsÞ  ð8Þ  n  ð9Þ  i¼0  which results in: n  YðsÞ ¼ ∑ GUY ðsÞGXUi ðsÞX i ðsÞ  ð10Þ  i¼0  This Eq. (10) can be simpliﬁed by substituting the following formula: Gi ðsÞ ¼ GUY GXUi ðsÞ  ð11Þ  It will be shown in Fig. 9 and it will be written now as: n  YðsÞ ¼ ∑ Gi ðsÞX i ðsÞ  ð12Þ  i¼0  i-th transfer function (transmittance) is given as: β  Gi ðsÞ ¼  i ∑l ¼ b sl 0 i;l αi ∑j ¼ 0 ai;j sj  ð13Þ  where: inj-  αi l-  βi -  The given vector of transform can also be written in the following form: GðsÞ ¼ ½G0 ðsÞ G1 ðsÞ … Gn ðsÞ  After putting Eq. (7) into (8) we can obtain: YðsÞ ¼ GUY ðsÞ ∑ GXUi ðsÞX i ðsÞ;  Fig. 11. The Laplace transform artiﬁcial neural network.  and the input vector: 2 3 X 0 ðsÞ 6 7 6 X 1 ðsÞ 7 7 XðsÞ ¼ 6 6 ⋮ 7 4 5 X n ðsÞ  ð14Þ  ð15Þ  Using the notation introduced previously in Eqs. (14) and (15), the formula (12) can be written as: YðsÞ ¼ GðsÞXðsÞ  ð16Þ  Learning process of LTAN is shown in Fig. 10. It consists of a lot of iteration modiﬁcation coefﬁcients ai;j and bi;l from Eq. (13) in order to minimize the value of ΔðsÞ, which is given as (Fig. 11):  ΔðsÞ ¼ ZðsÞ YðsÞ  ð17Þ  Z(s) is the known learning value of the output signal in response to input signals X(s). Y(s) is a real value of the output signal. The learning algorithm is represented by the block ID in Fig. 10. There are known a lot of different learning algorithms which will be adopted in the nearest future in order to be used for LTAN.  3. The Laplace transform neural network the number of the given input, the number of inputs, the number of the given power of factor of the polynomial in the denominator, the amount of the given power of factor of the polynomial in the denominator, the number of the given power of factor of the polynomial in the numerator, the amount of the given power of factor of the polynomial in the numerator.  By using previously presented artiﬁcial neurons (LTAN), it is possible to create a new kind of neural network – Laplace transform neural network. The input vector is: 2 3 X 0 ðsÞ 6 7 6 X 1 ðsÞ 7 7 ð18Þ XðsÞ ¼ 6 6 ⋮ 7 4 5 X n ðsÞ  358  P. Szymczyk, M. Szymczyk / Neurocomputing 148 (2015) 354–362  The output vector is: 2 3 Y 0 ðsÞ 6 7 6 Y 1 ðsÞ 7 7 YðsÞ ¼ 6 6 ⋮ 7 4 5 Y m ðsÞ The matrix of transform 2 F 0;0 ðsÞ F 0;1 ðsÞ ⋯ 6 6 F 1;0 ðsÞ F 1;1 ðsÞ ⋯ FðsÞ ¼ 6 6 ⋮ ⋮ ⋱ 4 F m;0 ðsÞ F m;1 ðsÞ ⋯  ð19Þ  of transfer function is: 3 F 0;n ðsÞ 7 F 1;n ðsÞ 7 7 ⋮ 7 5 F m;n ðsÞ  ð20Þ  Fig. 13. The supervised learning LTANN.  Using the notation from Eqs. (18), (19) and (20), YðsÞ ¼ FðsÞXðsÞ  ð21Þ  can be obtained. i-th, k-th transfer function (transmittance) is given as:  ΘðsÞ ¼ ZðsÞ YðsÞ  β  F i;k ðsÞ ¼  i;k ∑l ¼ b sl 0 ði;kÞ;l  αi;k  ∑j ¼ 0 aði;kÞ;j  ð22Þ  sj  where: ikj-  αi l-  βi -  Eq. (22) in order to minimize value of ΘðsÞ, which is given as:  the number of the given input, the number of the given output, the number of the given power of factor of the polynomial in the denominator, the amount of the given power of factor of the polynomial in the denominator, the number of the given power of factor of the polynomial in the numerator, the amount of the given power of factor of the polynomial in the numerator.  Z(s) is the known learning value of the output signal in response to input signals X(s). Y(s) is a real value of the output signal. The learning algorithm is represented by the block ID in Fig. 13. There are known a lot of different learning algorithms which will be adopted in the nearest future in order to be used for LTANN.  4. Examples of LTANN 4.1. Ordinary artiﬁcial neural networks If we assume that: bn;1 ¼ bn;2 ¼ ⋯ ¼ bn;βl and an;1 ¼ an;2 ¼ ⋯ ¼ an;αi for Eq. (13), i.e. only an;0 a 0 and bn;0 a0, the artiﬁcial neuron with Laplace transform becomes the simple artiﬁcial neuron with linear transfer function.  The simplest network LTANN consists of only one layer of neurons. All the neurons LTAN have inputs connected to external inputs of the network and the output of each neuron is directly connected to the external output of the network. Such a network is shown in Fig. 12. The network from Fig. 12 can be described by transmittance (transfer function) in the following form: 2 3 F 0 ðsÞ 6 7 6 F 1 ðsÞ 7 7 FðsÞ ¼ 6 ð23Þ 6 ⋮ 7 4 5 F m ðsÞ  where For  where  T μ ðsÞ ¼ e  μτs  F p ðsÞ ¼ ½F p;0 ðsÞ  F p;1 ðsÞ  …  F p;n ðsÞ  ð24Þ  The learning process of LTANN is shown in Fig. 13. It consists of a lot of iteration modiﬁcation coefﬁcients aði;kÞ;j and bði;kÞ;l from  ð25Þ  4.2. Time delay neural networks It can be noted that for time delay neural networks (Fig. 6) subsequent input of neuron is delayed with time τ. It is possible to describe this problem in LTANN as an additional factor in the transmittance of the given input. From the time shift property of Laplace transform for this problem the following formula is obtained: TðsÞ ¼ e  τs  ð26Þ  τ is time delay. μ-th time delay: ð27Þ  The transmittance of ith input of LTAN with μ-th time delay can be shown as: Gi T μ ðsÞ ¼ e  μτs Gi ðsÞ  ð28Þ  where Gi(s) is a original transmittance. 4.3. Band splitter Let us consider the network LTANN which is designed to split 3 bands of frequency (under ω1, between ω1 and ω2 and over ω1). The structure of this network is shown in Fig. 14. Transmittance of the passive second-order low-pass ﬁlter is given as: H L ðsÞ ¼ Fig. 12. One layer LTANN.  bL;0 aL;2 s2 þ aL;1 s þ aL;0  ð29Þ  P. Szymczyk, M. Szymczyk / Neurocomputing 148 (2015) 354–362  359  For the received signal for the soil without anomaly it is possible to calculate the interpolation as the polynomial function in the following form: vWA ðtÞ  1:01n10  244  2:16n10  241 t þ 1:59n10  238 t 2  4:04n10  236 t 3 þ ⋯ þ 0:07t 98  1:55t 99 þ 15:63t 100  61:75t 101  ð38Þ  The Laplace transform for the function from Eq. (38) is obtained: 1:01n10  244 2:16n10  241 1:59n10  238 4:04n10  236  þ  s s2 s3 s4  V WA ðsÞ  Fig. 14. The band splitter on networks LTANN.  þ⋯þ  0:07 1:55 15:63 61:75  99 þ 100  101 s98 s s s  Transmittance of the second-order band-pass ﬁlter is given as:  The transmittance from Eq. (36) in this case is given as:  bB;1 s H B ðsÞ ¼ aB;2 s2 þ aB;1 s þ aB;0  H WA ðsÞ ¼  ð30Þ  V WA ðsÞ X WA ðsÞ  ð40Þ  Transmittance of passive second-order low-high ﬁlter is given as:  If we put Eqs. (37) and (39) into (40), we obtain:  bH;2 s2 H H ðsÞ ¼ aH;2 s2 þ aH;1 s þaH;0  H WA ðsÞ   ð31Þ  In this example the parameters of transmittances were obtained from the symbolic calculation of transform, but generally, they can be determined by the learning process.  ð39Þ  1:01n10  244 2:16n10  241 1:59n10  238  þ s s2 s3 4:04n10  236 0:07 1:55 þ ⋯ þ 98  99 s s s4 !   11 8 15:63 61:75 2n10 ð1  e  5n10 s Þ þ 100  101 n s s s2 þ 4n1016   ð41Þ  5. The model of soil from radargram Using LTANN, it is possible to make the model of the soil in the following way. The transmitted signal: xðtÞ ¼ ηð sin ðωtÞ  sin ðωt  τÞHðωt  τÞÞ where H is the Heaviside step function. Transmittance of the transmitted signal:     ω ð1  e  τs Þ XðsÞ ¼ L xðtÞ ¼ η 2 s þ ω2  ð32Þ  n  The transmittance of the transmitted signal for the soil with the sinkhole is given as: X SH ðsÞ ¼  ð33Þ  The received signal can be interpolated by the polynomial: yðtÞ ¼ ∑ λi t i  5.2. Model of soil with a sinkhole  ð34Þ  ð35Þ  5.1. Model of soil without anomaly  s2 þ 4n1016   11  s  Þ   2:93n10  244 5:89n10  241 4:36n10  238 þ þ 2 s s s3  1:95n10  235 1:95 21:02 130:62 350:59 þ þ ⋯ þ  98 þ 99 þ  100 þ 101 s s s s s4  ð44Þ  The transmittance from Eq. (36) in this case is given as: H SH ðsÞ ¼  V SH ðsÞ X SH ðsÞ  ð45Þ  If we put Eqs. (42) and (44) into (45), we obtain: H SH ðsÞ     2:93n10  244 5:89n10  241 4:36n10  238 þ þ s s2 s3  1:95n10  235 1:95 21:02 þ þ⋯ þ  98 þ 99 þ s s s4  130:62 350:59 n  100 þ 101 s s !  11 2n108 ð1  e  5n10 s Þ þ  The transmittance of the transmitted signal for the soil without anomaly is given as: 2n108 ð1  e  5n10  ð43Þ  The Laplace transform for the function from Eq. (43) is obtained:  þ ð36Þ  ð42Þ  þ 1:95n10  235 t 3 þ þ ⋯ þ  1:95t 98  V SH ðsÞ   In this case, the learning process is the identiﬁcation of coefﬁcients λi from Eq. (36). In this way the model of a soil for a given geological structure can be obtained. For different geological structures (sinkholes, loose zones of the dike etc.) it is possible to prepare different models.  X WA ðsÞ ¼  Þ  þ 21:02t 99 þ  130:62t 100 þ 350:59t 101  λ  YðsÞ  ¼  HðsÞ ¼ ω XðsÞ η 2 ð1  e  τs Þ s þ ω2  s  s2 þ 4n1016  vSH ðtÞ   2:93n10  244 þ 5:89n10  241 t þ 4:36n10  238 t 2  The transfer function (the model) is given as: i! ∑ni¼ 0 i þi 1 s   11  For the signal for the soil with the sinkhole it is possible to calculate the interpolation as the polynomial function in the following form:  i¼0  Transmittance of the received signal: ( ) n n λ i!   YðsÞ ¼ L yðtÞ ¼ L ∑ λi t i ¼ ∑ i þi 1 i¼0 i¼0s  2n108 ð1  e  5n10  ð37Þ  n  s2 þ 4n1016  ð46Þ  360  P. Szymczyk, M. Szymczyk / Neurocomputing 148 (2015) 354–362  5.3. Model of soil with a loose zone The transmittance of the transmitted signal for the soil with the loose zone is given as: X LZ ðsÞ ¼  5n108 ð1  e  5n10   11  s  Þ  ð47Þ  s2 þ25n1016  For the signal for the soil with the loose zone it is possible to calculate the interpolation as the polynomial function in the following form: vLZ ðtÞ  4:02n10   248  þ 8:55n10   243  t þ  1:72n10   239 2   2:07n10 t   F L ZðsÞ ¼  H LS ðsÞ  0  0  1   ð54Þ  t  þ 9:63n10  237 t 3 þ þ ⋯ þ  187:13t 98 þ2:66n103 t 99 þ 4 100  Fig. 15. The LTANN for classiﬁcation of geological structure.  4 101  þ 6:60n10 t  ð48Þ  The Laplace transform for the function from Eq. (48) is obtained: 4:02n10  248 8:55n10  243 1:72n10  239 9:63n10  237 þ V LZ ðsÞ  þ þ þ 2 s s s3 s4  187:13 2:66n103 þ þ s98 s99 4 4 2:07n10 6:60n10 þ  s100 s101  DWA ðsÞ ¼ H WA XðsÞ  YðsÞ  ð55Þ  DSH ðsÞ ¼ H WA XðsÞ  YðsÞ  ð56Þ  DLZ ðsÞ ¼ H WA XðsÞ  YðsÞ  ð57Þ  The difference between the known model and the unknown geological structure provides a possibility to classify this geological structure.  þ⋯þ   ð49Þ  The transmittance from Eq. (36) in this case is given as: H LZ ðsÞ ¼  V LZ ðsÞ X LZ ðsÞ  ð50Þ  If we put Eqs. (47) and (49) into (50), we obtain: H LZ ðsÞ  ð4:02n10  248 þ 8:55n10  243 t þ  1:72n10  239 t 2 þ 9:63n10  237 t 3 þ ⋯ þ  187:13t 98 þ 2:66n103 t 99 þ  2:07n104 t 100 þ 6:60n104 t 101 Þn !  11 5n108 ð1  e  5n10 s Þ  n  s2 þ 25n1016  ð51Þ  6. Conclusion The idea behind this paper was to describe a new kind of artiﬁcial neuron – LTAN and a new kind of artiﬁcial neuron networks – LTANN, which use the Laplace transform instead of ordinary weights and the linear activation function of an artiﬁcial neuron. This kind of network is the generalization of the number of the known artiﬁcial networks. In the nearest future new effective learning algorithms of neuron LTAN and networks LTANN will be developed. Determination of G(s) from Eq. (13) and F(s) from Eq. (22) is the problem of identiﬁcation [38–41], similar to that from the ﬁeld of control systems. As an example of using LTAPP, the models of different geological structures based on the LTANN are shown. By using these models, it is possible to classify an unknown geological structure, for instance as, the structure without anomaly, the structure with the sinkhole or the structure with the loose zone.  5.4. Classiﬁcation of geological structure When the receive signal is known for an unknown geological structure, it is possible put it to LTANN shown in Fig. 15. Each of the outputs from this LTANN is the difference between the unknown signal and each of the reference signals. The following denotation is used in Fig. 15: X(s) the transmitted signal YUN(s) - the received signal for the unknown geological structure FWA(s) - the transmittance of LTAN represents the structure without anomaly FSH(s) - the transmittance of LTAN represents the structure with the sinkhole FLZ(s) - the transmittance of LTAN represents the structure with the loose zone DWA(s) - the difference of the unknown structure to the structure without anomaly DSH(s) - the difference of the unknown structure to structure with the sinkhole DLZ(s) - the difference of the unknown structure to the structure with the loose zone Bearing the above in mind, we can write:   0 H WA ðsÞ F WA ðsÞ ¼ ð52Þ 0 1  F SH ðsÞ ¼  H SH ðsÞ  0  0  1   ð53Þ  Acknowledgments The research was ﬁnanced from the funds of National Science Center Poland, on the basis of agreement no. UMO- 2011/01/B/ST7/ 06178 and decision no. DEC-2011/01/B/ST7/06178.  Appendix A A.1. The Laplace transform The Laplace transform [42–44] of a function f(t), deﬁned for all real numbers t Z 0, is the function F(s) with the complex argument s, deﬁned by: Z 1 FðsÞ ¼ Lff ðtÞg ¼ f ðtÞe  st dt ð58Þ 0  where s ¼ σ þ iω with real numbers σ and ω. The inverse Laplace transform is given by the following formula: Z σ þ iω   1 f ðtÞ ¼ L  1 FðsÞ ¼ FðsÞest ds ð59Þ 2 π i σ  iω There are three possibilities to calculate the inverse Laplace transform [43]:   Based on the Heaviside theorem  Using a simple fractions decomposition  Using the residual method  P. Szymczyk, M. Szymczyk / Neurocomputing 148 (2015) 354–362  Properties of the Laplace transform [45,46]:  The method of Laplace transform consists of the three following phases [45]:   Additivity: if Lff ðtÞg ¼ FðsÞ and LfgðtÞg ¼ GðsÞ, then Lff ðtÞ þ gðtÞg ¼ FðsÞ þ GðsÞ  ð60Þ   Constant multiple: if Lff ðtÞg ¼ FðsÞ and a is any constant number, then Lfaf ðtÞg ¼ aFðsÞ  361  ð61Þ   Linearity: if α; β ¼ const and Lff ðtÞg ¼ FðsÞ and LfgðtÞg ¼ GðsÞ,   Finding Laplace transforms of all components of equations  Solving obtained algebraic equations  Calculating inverse Laplace transforms This methods has very often been used for years to solve many problems in the ﬁelds of physics, control systems, electrics and electronics. The Laplace transform is applied in linear stationary continuous-time systems [46].  then Lfαf ðtÞ þ β gðtÞg ¼ αFðsÞ þ βGðsÞ  ð62Þ   Time scaling: if Lff ðtÞg ¼ FðsÞ and a 40, then   1 s	 L f ðatÞ ¼ F a a  ð63Þ   Complex shift: if Lff ðtÞg ¼ FðsÞ and a is constant number, then Lfe  at f ðtÞg ¼ Fðs þ aÞ  ð64Þ   Time shift: if Lff ðtÞg ¼ FðsÞ and a 4 0, then Lff ðt aÞg ¼ e  as FðsÞ  ð65Þ   N-th order differentiation: if Lff ðtÞg ¼ FðsÞ, then Lff  ðnÞ  n  ðtÞg ¼ sn FðsÞ  ∑ sk  a f  ðn  kÞ  ð0Þ  ð66Þ  k¼1   Integration: if Lff ðtÞg ¼ FðsÞ, then  Z L    t    FðsÞ s Multiplication by time: if Lff ðtÞg ¼ FðsÞ, then f ðτ Þ dτ  ¼  ð67Þ    FðsÞ L tf ðtÞ ¼  ds  ð68Þ  0   Periodic function: if f ðt þ TÞ ¼ f ðtÞ, then FðsÞ ¼  1 1  e  Ts  Z  T  f ðtÞe  st dt  ð69Þ  0   Convolution: if Lff ðtÞg ¼ FðsÞ and LfgðtÞg ¼ GðsÞ, then Lff ðtÞngðtÞg ¼ FðsÞGðsÞ  ð70Þ  where Z f ðtÞgðtÞ ¼ 0  t  f ðt  τÞgðtÞ dτ ¼  Z 0  t  f ðtÞgðt  τÞ dτ:  For continuous-time input signal x(t) and output y(t) the transfer function (called also transmittance) H(s) is the linear mapping of the Laplace transform of the input, XðsÞ ¼ LfxðtÞg, into the Laplace transform of the output YðsÞ ¼ LfyðtÞg: HðsÞ ¼  YðsÞ LfyðtÞg ¼ XðsÞ LfxðtÞg  ð71Þ  The transform turns integral equations and differential equations into polynomial equations, which are much easier to solve by formal rules of algebra [47].  References [1] J. Karczewski, L. Ortyl, M. Pasternak, Zarys Metody Georadarowej, Wydawnictwa AGH, Kraków, 2011. [2] H.M. Joy, Ground Penetrating Radar Theory and Applications, Elsevier, Amsterdam, 2009. [3] M. Szymczyk, P. Szymczyk, Preprocessing of GPR data, Image Process. Commun. 16 (2013) 83–90. [4] R.O. Duda, P.E. Hart, D.G. Stork, Pattern Classiﬁcation, Wiley, NJ, 2001. [5] R.E. Maleki, A. Rezaei, B. Minaei, Comparison of classiﬁcation methods based on the type of attributes and sample size, J. Converg. Inf. Technol. 4 (3). [6] M. Hagan, H. Demuth, M. Beale, Neural Network Design, Vikas Publishing House, Noida, 2003. [7] D. Graupe, Principles of Artiﬁcial Neural Networks, 3rd Edition, World Scientiﬁc Publishing Co Pte Ltd, Singapore, 2013. [8] R. Tadeusiewicz, New trends in neurocybernetics, Comput. Methods Mater. Sci. 10 (2010) 1–7. [9] R. Tadeusiewicz, R. Chaki, N. Chaki, Exploring Neural Networks with C#, CRC Press, Taylor & Francis Group, Boca Raton, 2014. [10] J.A. Anderson, Introduction to Neural Networks, MIT Press, Cambridge, MA, 1995. [11] D. Broomhead, D. Lowe, Multivariable functional interpolation and adaptive networks, Complex Syst. 2 (1988) 321–355. [12] T. Kohonen, T. Honkela, Kohonen network, Scholarpedia 2 (1) (2007) 1568. [13] R. Rojas, Neural Networks–A Systematic Introduction, Springer-Verlag, Berlin, 1996. [14] T. Kohonen, Self-Organizing Maps, Springer, Berlin, 1995. [15] D.E. Rumelhart, G.E. Hinton, R.J. Williams, Learning internal representations by error propagation, in: D.E. Rumelhart, J.L. Mcclelland (Eds.), Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Foundations, vol. 1, MIT Press, Cambridge, MA, 1986, pp. 318–362. [16] R.J. Williams, D. Zipser, Gradient-based learning algorithms for recurrent networks and their computational complexity, in: Back-propagation: Theory, Architectures and Applications, Erlbaum, Hillsdale, NJ, 1994. [17] J. Schmidhuber, A local learning algorithm for dynamic feedforward and recurrent networks, Connect. Sci. 1 (4) (1989) 403–412. [18] W.L.J.C. Principe, N.R. Euliano, Neural and Adaptive Systems: Fundamentals through Simulation, Wiley, NJ, 1999. [19] J. Schmidhuber, A ﬁxed size storage o(n3) time complexity learning algorithm for fully recurrent continually running networks, Neural Comput. 4 (2) (1992) 243–248. [20] B.A. Pearlmutter, Learning state space trajectories in recurrent neural networks, Neural Comput. 1 (2) (1989) 263–269. [21] H. Cruse, Neural Networks As Cybernetic Systems, Science Brieﬁngs Series, George Thieme Verlag, Stuttgart, 1996. [22] H. Jaeger, H. Haas, Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication, 2004. [23] W. Maass, T. Natschlager, H. Markram, A fresh look at real-time computation in generic recurrent neural circuits, Technical report, Institute for Theoretical Computer Science, TU Graz, 2002. [24] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural Comput. 9 (8) (1997) 1735–1780. [25] A. Graves, J. Schmidhuber, Ofﬂine handwriting recognition with multidimensional recurrent neural networks, in: D. Koller, D. Schuurmans, Y. Bengio, L. Bottou (Eds.), NIPS, Curran Associates, Inc., La Jolla, 2008, pp. 545–552. [26] F.A. Gers, J. Schmidhuber, LSTM recurrent networks learn simple context free and context sensitive languages, IEEE Trans. Neural Netw. 12 (6) (2001) 1333–1340. [27] M. Schuster, K.K. Paliwal, Bidirectional recurrent neural networks, IEEE Trans. Signal Process. 45 (1997) 2673–2681. [28] A. Graves, J. Schmidhuber, Framewise phoneme classiﬁcation with bidirectional LSTM and other neural network architectures, Neural Netw. 18 (2005) 602–610. [29] J. Schmidhuber, Learning complex extended sequences using the principle of history compression, Neural Comput. 4 (2) (1992) 234–242. [30] J.A. Anderson, E. Rosenﬁeld, Talking nets: an oral history of neural networks, IEEE Trans. Neural Netw. 9 (5) (1998) 1054. [31] E.M. Izhikevich, Polychronization: computation with spikes, Neural Comput. 18 (2) (2006) 245–282.  362  P. Szymczyk, M. Szymczyk / Neurocomputing 148 (2015) 354–362  [32] J.-M. KUO, Nonlinear Dynamic Modeling with Artiﬁcial Neural Networks, University of Florida, Florida, 1996. [33] F.J. Pineda, Dynamics and architecture for neural computation, J. Complex. 4 (1988) 216–245. [34] B.B. Nasution, A.I. Khan, A hierarchical graph neuron scheme for real-time pattern recognition, IEEE Trans. Neural Netw. 19 (2) (2008) 212–229. [35] J.B. Andrej Krenker, A. Kos, Introduction to the Artiﬁcial Neural Networks, Artiﬁcial Neural Networks - Methodological Advances and Biomedical Applications, InTech, Rijeka, 2011. [36] K.J. Hunt, D. Sbarbaro, R. Zbikowski, P.J. Gawthrop, Neural networks for control systems: a survey, Automatica 28 (6) (1992) 1083–1112. [37] N. Huang, D. Xu, X. Liu, L. Lin, Power quality disturbances classiﬁcation based on s-transform and probabilistic neural network, Neurocomputing 98 (2012) 12–23. [38] H. Wysocki, Zastosowanie nieklasycznego rachunku operatorów do identyﬁkacji liniowych układów dynamicznych, Akademicka Oﬁcyna Wydawnicza EXIT, Warszawa, 2006. [39] K. Mańczak, Z. Nahorski, Komputerowa identyﬁkacja obiektów dynamicznych, Państwowe Wydawnictwo Naukowe, Warszawa, 1983. [40] P. Eykhoff, Identyﬁkacja w układach dynamicznych, Państwowe Wydawnictwo Naukowe, Warszawa, 1980. [41] Y. Zhu, Multivariable System Identiﬁcation for Process Control, Elsevier Science Ltd., Oxford, 2001. [42] J. Williams, Laplace Transforms, George Allen & Unwin LTD, London, 1973. [43] A. Świetlicka, A. Rybarczyk, A. Jurkowlaniec, Rachunek operatorowy, Wydawnictwo Naukowe PWN, Warszawa, 2012. [44] M. Hazewinkel, Encyclopedia of Mathematics, Springer, Berlin, 2001. [45] D. Bobrowski, Z. Ratajczak, Przekształcenie Laplace'a i jego zastosowania, Wydawnictwo Uczelniene Politechniki Poznańskiej, Poźnan, 1974. [46] W. Ditkin, A. Prudnikow, Przekształcenie całkowe i rachunek operatorowy, Państwowe Wydawnictwo Naukowe, Warszawa, 1964. [47] F. Bierski, Funkcje zespolone, szeregi i przekształcenia Fouriera, przekształcenie całkowe Laplace'a, przekształcenie Laurenta (Z), Wydawnictwo AGH, Kraków, 1990.  Piotr Szymczyk earned his Masters degree in Electronic Engineering in AGH University of Science and Technology (Kraków, Poland) in 1988, and Ph.D. degree in Computer Science also in AGH University of Science and Technology in 1997. Currently, he is a lecturer at AGH University of Science and Technology. His research interests include real time computer systems, embedded systems, natural computing and bioinformatics.  Magdalena Szymczyk earned her Masters degree in Electronic Engineering in AGH University of Science and Technology (Kraków, Poland) in 1988, and Ph.D. degree in Computer Science also in AGH University of Science and Technology in 1999. Currently, she is a lecturer at AGH University of Science and Technology. Her research interests include real time computer systems, embedded systems, parallel programming and bioinformatics.  